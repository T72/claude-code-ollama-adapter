# ollama-openai-proxy
FastAPI proxy that exposes an OpenAI-compatible /v1/chat/completions endpoint, translating requests to Ollama's native /api/chat format (with think/reasoning support for GLM-5:cloud)
